import pandas as pd
import os
from datetime import datetime
import re

# ------------------------------------------------------------------------------------------------------------------------------------------------------

# region 1. --- CONFIGURATION ---

# On se place dynamiquement
current_dir = os.path.dirname(os.path.abspath(__file__))
# Si le script est dans un sous-dossier, on remonte. Sinon on reste l√†.
if "scrapers" in current_dir:
    project_root = os.path.dirname(os.path.dirname(current_dir)) # Remonte 2 fois (scrapers/nom_dossier)
    if "PathFinder" not in project_root: # S√©curit√© si structure diff√©rente
         project_root = os.path.dirname(current_dir)
else:
    project_root = current_dir

# Chemins des fichiers PROPRES
FILE_FT = os.path.join(project_root, "data", "clean", "offres_francetravail_clean.csv")
FILE_WTTJ = os.path.join(project_root, "data", "clean", "offres_wttj_clean.csv")
FILE_APEC = os.path.join(project_root, "data", "clean", "offres_apec_clean.csv")

OUTPUT_CSV = os.path.join(project_root, "data", "clean", "global_job_market.csv")

# Cr√©ation du dossier final s'il n'existe pas
os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

print("üß™ D√©marrage de la fusion...")
# endregion

# ======================================================================================================================================================

#region 2. --- FONCTIONS ---

def normaliser_date(date_str):
    """Force le format AAAA-MM-JJ pour √©viter les bugs Streamlit"""
    if pd.isna(date_str) or date_str == "" or str(date_str).lower() == "nan":
        return None
    date_str = str(date_str).strip()
    try:
        # Tente le format fran√ßais (31/01/2025)
        return datetime.strptime(date_str, "%d/%m/%Y").strftime("%Y-%m-%d")
    except ValueError:
        try:
            # Tente le format d√©j√† ISO (2025-01-31)
            return datetime.strptime(date_str, "%Y-%m-%d").strftime("%Y-%m-%d")
        except ValueError:
            return None
# --------------------------------------------------        

def extraire_annees_exp(description):
    """
    Extrait le nombre d'ann√©es d'exp√©rience du texte.
    Renvoie un entier ou None.
    """
    if pd.isna(description): return None
    description = description.lower()

    annees = None
    #pattern_a = r"(\d{1,2})\s*?(?:-|\s)?\s*?(?:ans|ann√©es|years|year).*?(?:exp|exp√©rience|experience)"  
    pattern_a = r"(\d{1,2})\s*(?:-|√†)?\s*(?:\d{1,2})?\s*(?:ans|ann√©es|years).{0,20}exp"  
    match_a = re.search(pattern_a, description)
    #pattern_b = r"(?:exp|exp√©rience|experience).{0,20}?(\d{1,2})"
    pattern_b = r"exp.{0,20}(\d{1,2})\s*(?:ans|ann√©es|years)"
    match_b = re.search(pattern_b, description)

    if match_a:
        try: annees = int(match_a.group(1))
        except ValueError: pass
    elif match_b:
        try: annees = int(match_b.group(1))
        except ValueError: pass
        
    # Filtre de s√©curit√© pour √©viter les chiffres aberrants
    if annees is not None and (annees > 15 or annees < 0):
        return None
        
    return annees
# --------------------------------------------------

def nettoyer_contrats(df):
    """
    Nettoie et standardise la colonne Type_Contrat.
    1. Formate le texte (Capitalize)
    2. Traduit les codes administratifs (Ddi -> CDD)
    3. Corrige intelligemment selon le Titre (Si Titre='Stage' -> Contrat='Stage')
    """
    print("‚ú® Nettoyage et Harmonisation des Contrats...")
    
    # 1. Nettoyage de base : String + Strip + Capitalize
    df["Type_Contrat"] = df["Type_Contrat"].astype(str).str.strip().str.capitalize()

    # 2. Dictionnaire de traduction (Codes -> Libell√©s propres)
    corrections_contrat = {
        "Mis": "Int√©rim",
        "Lib": "Freelance",
        "Fra": "Freelance",
        "Ind": "Freelance",
        "Din": "CDI Int√©rimaire",
        "Cdi": "CDI",
        "Cdd": "CDD",
        "Ddi": "CDD",  # Contrat √† Dur√©e D√©termin√©e d'Insertion
        "Cui": "CDD",  # Contrat Unique Insertion
        "Cae": "CDD",  # Contrat d'accompagnement dans l'emploi
        "Stage": "Stage / Alternance",
        "Alternance": "Stage / Alternance",
        "Apprentissage": "Stage / Alternance",
        "Contrat pro": "Stage / Alternance",
        "Stage / alternance": "Stage / Alternance",
        "Professionalisation": "Stage / Alternance",
        "Nan": "Non sp√©cifi√©"
    }
    df["Type_Contrat"] = df["Type_Contrat"].replace(corrections_contrat)

    # --- PR√âPARATION DES MASQUES (Les "D√©tecteurs") ---

    # A. D√âTECTEUR SENIOR / MANAGER (Liste Noire pour Stage)
    regex_titre_senior = r"\b(?:senior|lead|manager|directeur|head of|chef de projet|international|freelance|expert|responsable)\b"
    mask_titre_senior = df['Titre'].astype(str).str.contains(regex_titre_senior, case=False, regex=True, na=False)

    # B. D√âTECTEUR √âTUDIANT
    regex_etudiants = r"\b(?:stage|stagiaire|internship|alternance|alternant|apprentissage|contrat pro|pfe)\b"    
    mask_titre_etudiant = df['Titre'].astype(str).str.contains(regex_etudiants, case=False, regex=True, na=False)

    # C. D√âTECTEUR CDI CACH√â (Le plus complexe)
    # 1. Inclusion : On cherche "CDI" ou "Dur√©e ind√©termin√©e"
    regex_cdi = r"\b(?:CDI|dur√©e ind√©termin√©e)\b"
    mask_contient_cdi = df['Description'].astype(str).str.contains(regex_cdi, case=False, regex=True, na=False)

    # 2. Exclusion : On fuit "possibilit√© de CDI", "vue sur CDI", etc.
    regex_cdi_piege = r"(?:possibilit|perspective|d√©bouch|vue|objectif|finalit√©|suite|embauche|stage|futur|apr√®s).{0,30}\bCDI\b"
    mask_cdi_piege = df['Description'].astype(str).str.contains(regex_cdi_piege, case=False, regex=True, na=False)

    # 3. R√©sultat : C'est un VRAI CDI Cach√©
    mask_vrai_cdi_cache = mask_contient_cdi & (~mask_cdi_piege)


    # --- APPLICATION DES R√àGLES (Ordre Chronologique) ---

    # √âTAPE 1 : ON APPLIQUE "STAGE" (Si Titre OK + Pas Senior + Pas CDI Cach√© + Exp Faible)
    # On ne force le stage que si tous les feux sont verts.
    mask_valid_stage = (
        mask_titre_etudiant & 
        (~mask_titre_senior) & 
        (~mask_vrai_cdi_cache) & 
        ((df['Annees_Exp'].isna()) | (df['Annees_Exp'] < 2))
    )
    df.loc[mask_valid_stage, 'Type_Contrat'] = "Stage / Alternance"


    # √âTAPE 2 : ON CORRIGE LES FAUX STAGES
    # Si c'est marqu√© "Stage" (Source ou Etape 1) MAIS que c'est un Senior ou Exp > 2 ans
    mask_is_stage = (df['Type_Contrat'] == "Stage / Alternance")
    mask_faux_stages = mask_is_stage & (mask_titre_senior | (df['Annees_Exp'] > 2))
    
    if mask_faux_stages.sum() > 0:
        print(f"   - üßπ Correction de {mask_faux_stages.sum()} faux stages (Seniors/Experts)... -> Passage en CDI")
        df.loc[mask_faux_stages, 'Type_Contrat'] = "CDI" # On assume CDI par d√©faut pour un Senior


    # √âTAPE 3 : ON R√âV√àLE LES CDIS CACH√âS üïµÔ∏è‚Äç‚ôÇÔ∏è
    # Si le contrat est "Non sp√©cifi√©", "CDD" ou m√™me "Stage" (sauf si Titre Etudiant explicite)
    # ET qu'on a d√©tect√© un VRAI CDI dans la description
    mask_candidats_cdi = df['Type_Contrat'].isin(["Stage / Alternance", "Non sp√©cifi√©", "CDD", "Int√©rim"])
    
    # On ne touche pas si le titre crie "Stage" (Ex: "Stage Assistant RH - CDI √† la cl√©")
    mask_correction_cdi = mask_candidats_cdi & mask_vrai_cdi_cache & (~mask_titre_etudiant)

    if mask_correction_cdi.sum() > 0:
        print(f"   - üõ°Ô∏è  Transformation de {mask_correction_cdi.sum()} offres en CDI (D√©tect√© dans la description sans ambigu√Øt√©)...")
        df.loc[mask_correction_cdi, 'Type_Contrat'] = "CDI"

    # Correction Freelance
    regex_freelance = r"\b(?:freelance|ind√©pendant|independant|free-lance|b2b)\b"
    mask_freelance = df['Titre'].astype(str).str.contains(regex_freelance, case=False, regex=True, na=False)
    df.loc[mask_freelance, 'Type_Contrat'] = "Freelance"

    return df    
# --------------------------------------------------

def determiner_niveau(row):
    """
    D√©duit le niveau d'exp√©rience (Etudiant, Junior, Confirm√©, Senior) 
    bas√© sur le salaire (distingue idf des autres zones) et les mots-cl√©s du titre.
    """
    titre = str(row['Titre']).lower() if pd.notna(row['Titre']) else ""
    # esc = str(row['Description']).lower() if pd.notna(row['Description']) else "" 
    lieu = str(row['Ville']).lower() if pd.notna(row['Ville']) else "" 
    salaire = row['Salaire_Annuel']
    contrat = str(row['Type_Contrat']).lower() if pd.notna(row['Type_Contrat']) else ""
    annees = row['Annees_Exp']

    # 1. Extraction des ann√©es
    
    if pd.notna(annees):
        if annees > 5: return "Senior"
        if annees > 2: return "Confirm√©"

    # 2. D√©tection des stages ---
    if "Stage / Alternance" in contrat:
        return "En formation"
    
    if any(k in titre for k in ["senior", "lead", "manager", "head of", "directeur", "expert", "principal", "vp", "chef"]):
        return "Senior"       

    # 3. D√©finition des seuils selon la g√©ographie    
    # Zone A : Paris & IDF
    mots_idf = ['paris', '√Æle-de-france', 'ile-de-france', 'boulogne', 'courbevoie', 'la d√©fense', '92', '75', '93', '94']
    
    # Zone B : Grandes M√©tropoles (March√© dynamique)
    mots_metropoles = ['lyon', 'toulouse', 'bordeaux', 'nantes', 'lille', 'aix', 'marseille', 'nice', 'rennes', 'sophia', 'antipolis']

    if any(m in lieu for m in mots_idf):
        # ZONE PARIS
        seuil_junior = 40000
        seuil_senior = 60000
    elif any(m in lieu for m in mots_metropoles):
        # ZONE GRANDES VILLES (Interm√©diaire)
        seuil_junior = 37000  # Un junior √† Lyon peut toucher 36-37k
        seuil_senior = 52000  # 52k √† Bordeaux, c'est clairement un profil Senior
    else:
        # ZONE RESTE DE LA FRANCE
        seuil_junior = 34000
        seuil_senior = 48000

    # --- VERDICT DU SALAIRE ---
    if pd.notna(salaire) and salaire > 0:
        if salaire <= seuil_junior:
            return "Junior"
        elif seuil_junior < salaire < seuil_senior:
            return "Confirm√©"
        else:
            return "Senior"    
    
    # --- PAR DEFAUT ---
    if pd.notna(annees) and annees <= 2:
        return "Junior"
    
    # --- FALLBACK : ANALYSE TEXTUELLE ---
    # (Titre)
    
    if any(k in titre for k in ["junior", "d√©butant", "assistant", "graduate"]):
        return "Junior"
    if "confirm√©" in titre:
        return "Confirm√©"    

    return "Non sp√©cifi√©"
# --------------------------------------------------

def detecter_rqth(text):
    if pd.isna(text): return False
    keywords = ["rqth", "handicap", "situation de handicap", "entreprise adapt√©e"]
    return any(k in text.lower() for k in keywords)

# endregion
# ======================================================================================================================================================

# region 3. --- CHARGEMENT ET STANDARDISATION ---

dataframes = []
cols_globales = [
    "Titre", "Entreprise", "Ville", "Salaire_Annuel", "Type_Contrat", 
    "Teletravail", "Date_Publication", "Date_Expiration", "Source", "URL", "Description"
]


# --- CHARGEMENT DE L'HISTORIQUE ---
if os.path.exists(OUTPUT_CSV):
    print(f"üìú Chargement de l'historique : {OUTPUT_CSV}")
    try:
        df_hist = pd.read_csv(OUTPUT_CSV)
        # On normalise aussi l'historique pour √™tre s√ªr
        df_hist["Date_Publication"] = df_hist["Date_Publication"].apply(normaliser_date)
        df_hist["Date_Expiration"] = df_hist["Date_Expiration"].apply(normaliser_date)
        dataframes.append(df_hist)
    except:
        print("‚ö†Ô∏è Historique illisible, on repart de z√©ro.")

# --------------------------------------------------

# --- A. FRANCE TRAVAIL ---
if os.path.exists(FILE_FT):
    print("üîπ Chargement France Travail...")
    df_ft = pd.read_csv(FILE_FT)
    # Renommage pour standardiser
    df_ft = df_ft.rename(columns={
        "Ville_Clean": "Ville",
        "Salaire_Annuel_Estime": "Salaire_Annuel",
        "Description_Propre": "Description"
    })
    # Ajout colonnes manquantes
    df_ft["Teletravail"] = "Non sp√©cifi√©" 
    df_ft["Date_Publication"] = df_ft["Date_Publication"].apply(normaliser_date)
    df_ft["Date_Expiration"] = df_ft["Date_Expiration"].apply(normaliser_date)
    

    # On g√®re si certaines colonnes manquent dans le CSV source
    for c in cols_globales:
        if c not in df_ft.columns: df_ft[c] = None
    dataframes.append(df_ft[cols_globales])
else:
    print("‚ö†Ô∏è Fichier France Travail introuvable !")

# --- B. WTTJ ---
if os.path.exists(FILE_WTTJ):
    print("üîπ Chargement WTTJ...")
    df_wttj = pd.read_csv(FILE_WTTJ)
    
    df_wttj = df_wttj.rename(columns={
        "Ville_Clean": "Ville",
        "Salaire_Annuel_Estime": "Salaire_Annuel",
        "Description_Propre": "Description",
        "Date": "Date_Publication"
    })
    
    # Ajout Source et Date (Aujourd'hui)
    df_wttj["Source"] = "Welcome to the Jungle"
    df_wttj["Date_Publication"] = df_wttj["Date_Publication"].apply(normaliser_date)    
    df_wttj["Date_Expiration"] = df_wttj["Date_Expiration"].apply(normaliser_date)
    
    for c in cols_globales:
        if c not in df_wttj.columns: df_wttj[c] = None
    dataframes.append(df_wttj[cols_globales])
else:
    print("‚ö†Ô∏è Fichier WTTJ introuvable !")

# --- C. APEC ---
if os.path.exists(FILE_APEC):
    print("üîπ Chargement APEC...")
    df_apec = pd.read_csv(FILE_APEC)
    
    df_apec = df_apec.rename(columns={
        "Ville_Clean": "Ville",
        "Salaire_Annuel_Estime": "Salaire_Annuel",
        "Description_Propre": "Description",
        "Date": "Date_Publication"
    })
    
    df_apec["Source"] = "Apec"    
    df_apec["Teletravail"] = "Non sp√©cifi√©"
    df_apec["Date_Publication"] = df_apec["Date_Publication"].apply(normaliser_date)
    df_apec["Date_Expiration"] = df_apec["Date_Expiration"].apply(normaliser_date)
    
    for c in cols_globales:
        if c not in df_apec.columns: df_apec[c] = None
    dataframes.append(df_apec[cols_globales])
else:
    print("‚ö†Ô∏è Fichier APEC introuvable !")
# endregion

# ======================================================================================================================================================

# region 4. --- FUSION ---

if not dataframes:
    print("‚ùå Aucun fichier charg√©. Arr√™t.")
    exit()

print("üå™Ô∏è  M√©lange des donn√©es...")
df_final = pd.concat(dataframes, ignore_index=True)

# === CORRECTION DE L'ANCIENNET√â (FIX DUR√âE DE VIE) ===

df_final['Date_Publication'] = pd.to_datetime(df_final['Date_Publication'], errors='coerce')
df_final['Date_Publication'] = df_final.groupby('URL')['Date_Publication'].transform('min')

print("üßπ Standardisation des grandes villes (Arrondissements)...")

df_final['Ville'] = df_final['Ville'].astype(str).str.replace(r'(?i)^paris.*', 'Paris', regex=True)
df_final['Ville'] = df_final['Ville'].str.replace(r'(?i)^lyon.*', 'Lyon', regex=True)
df_final['Ville'] = df_final['Ville'].str.replace(r'(?i)^marseille.*', 'Marseille', regex=True)

# Correction sp√©cifique pour "La D√©fense" qui appara√Æt parfois comme "Puteaux" ou "Courbevoie"
# (Optionnel, mais utile pour regrouper les offres de ce hub)
# df_final['Ville'] = df_final['Ville'].replace(['Courbevoie', 'Puteaux', 'Nanterre'], 'La D√©fense')

# --------------------------------------------------

# --- Application des corrections ---

print("‚ú® Nettoyage des guillemets r√©siduels...")
cols_text = ['Titre', 'Entreprise', 'Ville']
for col in cols_text:
    # On force en string, on remplace les " et on enl√®ve les espaces vides
    df_final[col] = df_final[col].astype(str).str.replace('"', '', regex=False).str.strip()
    
    # On enl√®ve les "nan" qui apparaissent parfois lors de la conversion string
    df_final[col] = df_final[col].replace('nan', 'Non sp√©cifi√©')

# Suppression des doublons (bas√© sur l'URL)
len_avant = len(df_final)
df_final = df_final.drop_duplicates(subset=["URL"], keep='last')
len_apres = len(df_final)

print(f"üßπ Doublons supprim√©s : {len_avant - len_apres}")

# === NETTOYAGE CONTRATS ===

print("üßÆ Extraction des ann√©es d'exp√©rience...")
df_final['Annees_Exp'] = df_final['Description'].apply(extraire_annees_exp)
df_final = nettoyer_contrats(df_final)

# === CALCUL DU NIVEAU D'EXP√âRIENCE ===
print("üß† Calcul des niveaux d'exp√©rience (Analyse Salaires & Texte)...")
# On applique la fonction ligne par ligne (axis=1)
df_final['Niveau'] = df_final.apply(determiner_niveau, axis=1)

# === STACKS ===

keywords = {
        "Python": r"\bpython\b",
        "SQL": r"\bsql\b",
        "Excel": r"\bexcel\b",
        "Power BI": r"power\s?bi", # Accepte "PowerBI" ou "Power BI"
        "Tableau": r"\btableau\b",
        "R": r"\bR\b",             # Attention, peut capter "R&D", mais souvent OK
        "SAS": r"\bsas\b",
        "VBA": r"\bvba\b",
        "AWS": r"\baws\b",
        "Azure": r"\bazure\b",
        "GCP": r"\bgcp\b|google\scloud",
        "Spark": r"\bspark\b",
        "Hadoop": r"\bhadoop\b",
        "Kafka": r"\bkafka\b",
        "Airflow": r"\bairflow\b",
        "Snowflake": r"\bsnowflake\b",
        "Databricks": r"\bdatabricks\b",
        "Docker": r"\bdocker\b",
        "Kubernetes": r"\bkubernetes\b|k8s",
        "Git": r"\bgit\b",
        "Linux": r"\blinux\b",
        "Pandas": r"\bpandas\b",
        "TensorFlow": r"\btensorflow\b",
        "PyTorch": r"\bpytorch\b",
        "Scikit-learn": r"scikit[\s\-]learn|sklearn",
        "Java": r"\bjava\b",       # Exclut Javascript gr√¢ce aux \b
        "Scala": r"\bscala\b",
        "C++": r"\bc\+\+",
        "NoSQL": r"no\s?sql|mongo|cassandra",
        "Dbt": r"\bdbt\b"
    }

def detecter_stack(description):
    if pd.isna(description): return ""
    desc_lower = str(description).lower()
    found = []
    for tech, pattern in keywords.items():        
        if re.search(pattern, desc_lower):
            found.append(tech)
            
    return ", ".join(found)

print("üß† Analyse des comp√©tences Tech...")
df_final['Tech_Stack'] = df_final['Description'].apply(detecter_stack)

# === INCLUSION ===

df_final['Handicap_Friendly'] = df_final['Description'].apply(detecter_rqth)

# endregion
# ======================================================================================================================================================

# region 5. --- SAUVEGARDE ---
df_final.to_csv(OUTPUT_CSV, index=False)

print(f"\n‚úÖ TERMIN√â ! Le fichier global est pr√™t :")
print(f"üëâ {OUTPUT_CSV}")
print("\nüìä STATISTIQUES FINALES :")
print(df_final["Source"].value_counts())
print(f"\nüí∞ Offres avec salaire : {df_final['Salaire_Annuel'].notna().sum()}")
# endregion