import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
import os
import json

# --- 0. CONFIGURATION ---
# Calcul automatique des chemins pour √©viter les erreurs
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))

INPUT_CSV = os.path.join(project_root, "data", "raw", "offres_wttj_complet_url.csv")
OUTPUT_CSV = os.path.join(project_root, "data", "enriched", "offres_wttj_full.csv")

# --- 1. CHARGEMENT ---
if not os.path.exists(INPUT_CSV):
    print(f"‚ùå ERREUR : Le fichier {INPUT_CSV} est introuvable.")
    exit()

df_source = pd.read_csv(INPUT_CSV)
# Pour tester :
#df_source = df_source.head(5) 

print(f"‚úÖ Chargement de {len(df_source)} offres.")

# --- 2. INIT FICHIER SORTIE ---
# On pr√©pare les colonnes pr√©cises que tu veux
colonnes = ["Titre", "Entreprise", "Ville", "Experience_Salaire_Infos", "Description_Complete", "URL"]

if not os.path.exists(OUTPUT_CSV):
    pd.DataFrame(columns=colonnes).to_csv(OUTPUT_CSV, index=False)
    deja_faites = []
else:
    deja_faites = pd.read_csv(OUTPUT_CSV)["URL"].tolist()

# --- 3. LE ROBOT ---
options = webdriver.ChromeOptions()
options.add_argument("--disable-blink-features=AutomationControlled")
# options.add_argument("--headless") # Laisse comment√© pour surveiller

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
driver.set_window_size(1920, 1080)

print("üöÄ D√©marrage de l'extraction...")

# --- 4. BOUCLE ---
for index, row in df_source.iterrows():
    url = row['URL']
    titre_csv = row['Titre']
    
    if url in deja_faites:
        print(f"‚è© D√©j√† fait : {titre_csv}")
        continue
    
    print(f"\nüîé ({index + 1}/{len(df_source)}) {titre_csv}")
    
    try:
        driver.get(url)
        time.sleep(random.uniform(4, 7)) # Pause n√©cessaire
        
        # Scroll pour charger tout le texte
        driver.execute_script("window.scrollBy(0, 600);")
        time.sleep(1)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # --- A. ENTREPRISE (Via URL - Infaillible) ---
        try:
            entreprise = url.split('/companies/')[1].split('/')[0].replace('-', ' ').upper()
        except:
            entreprise = "INCONNU"

        # --- B. LES INFOS CL√âS (Ville, Exp√©rience, Salaire) ---
        # Sur WTTJ, ces infos sont souvent dans une liste <ul> avec des ic√¥nes juste sous le titre.
        # On va r√©cup√©rer TOUS les √©l√©ments de cette liste et les mettre dans une colonne.
        infos_cles = []
        ville = "Non sp√©cifi√©"
        
        # 1. On cherche d'abord dans le code cach√© pour Google (JSON-LD)
        # C'est la source la plus fiable pour la localisation pr√©cise
        try:
            script_json = soup.find('script', type='application/ld+json')
            if script_json:
                data = json.loads(script_json.string)
                
                # Parfois le JSON est une liste, parfois un dictionnaire unique
                if isinstance(data, list):
                    # On cherche l'objet qui est une offre d'emploi
                    job_data = next((item for item in data if item.get('@type') == 'JobPosting'), None)
                else:
                    job_data = data if data.get('@type') == 'JobPosting' else None
                
                if job_data and 'jobLocation' in job_data:
                    address = job_data['jobLocation'].get('address', {})
                    # On r√©cup√®re la ville propre
                    ville_json = address.get('addressLocality')
                    if ville_json:
                        ville = ville_json
                        print(f"   üéØ Ville trouv√©e (JSON) : {ville}")
        except Exception as e:
            # Si le JSON √©choue, pas grave, on continue avec la m√©thode visuelle
            pass

        # 2. On r√©cup√®re quand m√™me les tags visuels (Contrat, Rythme, Salaire...)
        # Car le JSON ne contient pas toujours le salaire ou le t√©l√©travail de fa√ßon claire
        try:
            tous_li = soup.find_all('li')
            for li in tous_li:
                texte = li.get_text(strip=True)
                # On ne garde que les "petits" textes (tags)
                if 0 < len(texte) < 50:
                    infos_cles.append(texte)
                    
                    # Si la m√©thode JSON a √©chou√© (ville toujours "Non sp√©cifi√©")
                    # On essaie de deviner la ville ici en secours
                    if ville == "Non sp√©cifi√©":
                        if "Paris" in texte or "Lyon" in texte or "Marseille" in texte or "Lille" in texte or "Bordeaux" in texte or "Nantes" in texte or "Toulouse" in texte:
                             ville = texte
        except:
            pass

        # On transforme la liste en une cha√Æne de texte propre (ex: "CDI | Paris | > 3 ans | 45k‚Ç¨")
        infos_concatenees = " | ".join(infos_cles)

        # --- C. LA DESCRIPTION (Comp√©tences & Missions) ---
        # On cherche le gros bloc de texte. 
        # Strat√©gie : On cherche la balise qui contient le mot "Descriptif" ou "Profil"
        description = "Non trouv√©e"
        
        # On cherche tous les paragraphes et les titres
        # C'est la m√©thode "Aspirateur" : on prend tout le contenu textuel pertinent
        main_content = soup.find('main')
        if main_content:
            # On prend le texte en gardant les sauts de ligne pour que ce soit lisible
            description = main_content.get_text(separator="\n", strip=True)
        else:
            # Plan B : Si pas de main, on cherche section par section
            sections = soup.find_all('section')
            textes_sections = [s.get_text(separator="\n", strip=True) for s in sections]
            # On garde la plus longue section (c'est forc√©ment la description)
            if textes_sections:
                description = max(textes_sections, key=len)

        # --- SAUVEGARDE ---
        nouvelle_ligne = {
            "Titre": titre_csv,
            "Entreprise": entreprise,
            "Ville": ville,
            "Experience_Salaire_Infos": infos_concatenees, # C'est ICI que tu auras l'XP et le Salaire
            "Description_Complete": description,
            "URL": url
        }
        
        df_new = pd.DataFrame([nouvelle_ligne])
        df_new.to_csv(OUTPUT_CSV, mode='a', header=False, index=False)
        
        # Petit feedback visuel
        print(f"   üìç Ville: {ville}")
        print(f"   üíº Infos: {infos_concatenees[:60]}...") # Affiche le d√©but des infos
        print("   ‚úÖ Sauvegard√©.")

    except Exception as e:
        print(f"‚ùå Erreur : {e}")

driver.quit()